{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Finetuning RoBERTa for NER: Compile Corpus"]},{"cell_type":"markdown","metadata":{},"source":["***"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:17:23.428297Z","iopub.status.busy":"2022-09-04T17:17:23.427338Z","iopub.status.idle":"2022-09-04T17:17:31.843100Z","shell.execute_reply":"2022-09-04T17:17:31.842031Z","shell.execute_reply.started":"2022-09-04T17:17:23.427831Z"},"trusted":true},"outputs":[],"source":["from transformers import (BertTokenizerFast,\n","                          RobertaTokenizerFast,\n","                          AutoTokenizer,\n","                          BertForTokenClassification,\n","                          RobertaForTokenClassification,\n","                          DataCollatorForTokenClassification, \n","                          AutoModelForTokenClassification, \n","                          TrainingArguments, Trainer)\n","from datasets import load_dataset, concatenate_datasets, DatasetDict\n","import pickle\n","import torch\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["## Load Tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["**Load Model and Tokenizer:**\n","\n","Information about model variants can be found here: https://huggingface.co/docs/transformers/model_doc/roberta"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:17:33.656105Z","iopub.status.busy":"2022-09-04T17:17:33.655459Z","iopub.status.idle":"2022-09-04T17:19:25.869219Z","shell.execute_reply":"2022-09-04T17:19:25.868230Z","shell.execute_reply.started":"2022-09-04T17:17:33.656069Z"},"trusted":true},"outputs":[],"source":["model_name = \"xlm-roberta-large\" #\"bert-base-multilingual-cased\" #xlm-roberta-large\n","tokenizer = AutoTokenizer.from_pretrained(f\"{model_name}\", add_prefix_space=True) #AutoTokenizer(use_fast = True)\n","#model = AutoModelForTokenClassification.from_pretrained(f\"{model_name}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Download Dataset for Finetuning"]},{"cell_type":"markdown","metadata":{},"source":["See:\n","* Dataset on Huggingface: https://huggingface.co/datasets/wikiann\n","* Load Datasets: https://huggingface.co/docs/datasets/v2.4.0/en/package_reference/loading_methods"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:19:26.633718Z","iopub.status.busy":"2022-09-04T17:19:26.633239Z","iopub.status.idle":"2022-09-04T17:20:00.173604Z","shell.execute_reply":"2022-09-04T17:20:00.172664Z","shell.execute_reply.started":"2022-09-04T17:19:26.633679Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"81a1f0a391a244b5b1e5029b999486d5","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/3.94k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98663b6e60114223b984d2926c35e5ae","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/12.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset wikiann/en (download: 223.17 MiB, generated: 8.88 MiB, post-processed: Unknown size, total: 232.05 MiB) to C:\\Users\\julia\\.cache\\huggingface\\datasets\\wikiann\\en\\1.1.0\\4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf5adb4115a449c8b02f6369ec0fa399","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/234M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5617a9d8f48c4e59a946f2c1b280bdcf","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ca5565cbe9e4decbf0dc90fdcad3760","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"837f3638d101402a853d75d961f84eab","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset wikiann downloaded and prepared to C:\\Users\\julia\\.cache\\huggingface\\datasets\\wikiann\\en\\1.1.0\\4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e. Subsequent calls will reuse this data.\n"]},{"name":"stderr","output_type":"stream","text":["Reusing dataset wikiann (C:\\Users\\julia\\.cache\\huggingface\\datasets\\wikiann\\en\\1.1.0\\4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e)\n","Reusing dataset wikiann (C:\\Users\\julia\\.cache\\huggingface\\datasets\\wikiann\\en\\1.1.0\\4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e)\n"]},{"name":"stdout","output_type":"stream","text":["Download Dataset for Language de\n","Downloading and preparing dataset wikiann/de (download: 223.17 MiB, generated: 10.51 MiB, post-processed: Unknown size, total: 233.67 MiB) to C:\\Users\\julia\\.cache\\huggingface\\datasets\\wikiann\\de\\1.1.0\\4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d6ad8b4833f4c8882d223da6ade66e9","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"800b464b64fa4b63b8fc122564704e9b","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"20248df5e9654162a3cefb632c415a96","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/20000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset wikiann downloaded and prepared to C:\\Users\\julia\\.cache\\huggingface\\datasets\\wikiann\\de\\1.1.0\\4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e. Subsequent calls will reuse this data.\n"]},{"name":"stderr","output_type":"stream","text":["Reusing dataset wikiann (C:\\Users\\julia\\.cache\\huggingface\\datasets\\wikiann\\de\\1.1.0\\4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e)\n","Reusing dataset wikiann (C:\\Users\\julia\\.cache\\huggingface\\datasets\\wikiann\\de\\1.1.0\\4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e)\n"]}],"source":["# Specify list of languages\n","languages = [\"en\",\"de\", \"fr\", \"es\", \"zh\"]\n","languages = [\"en\", \"de\"]\n","#languages = [\"en\"]\n","dataset_name = \"wikiann\"\n","\n","# Downloa first language\n","dataset_train = load_dataset(dataset_name, languages[0],  split=\"train\")\n","dataset_valid = load_dataset(dataset_name, languages[0],  split=\"validation\")\n","dataset_test =  load_dataset(dataset_name, languages[0],  split=\"test\")\n","languages.pop(0)\n","\n","# Merge with additional languages\n","for language in languages:\n","    \n","    print(f\"Download Dataset for Language {language}\")\n","    \n","    # Combine train splits\n","    dataset_train_new = load_dataset(dataset_name, language,  split=\"train\")\n","    dataset_train = concatenate_datasets([dataset_train, dataset_train_new])\n","\n","    # Combine validation splits\n","    dataset_valid_new = load_dataset(dataset_name, language,  split=\"validation\")\n","    dataset_valid = concatenate_datasets([dataset_valid, dataset_valid_new])\n","    \n","    # Combine test splits\n","    dataset_test_new = load_dataset(dataset_name, language,  split=\"test\")\n","    dataset_test = concatenate_datasets([dataset_test, dataset_test_new])"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["dataset = DatasetDict({\n","    \"train\":dataset_train,\n","    \"test\":dataset_test, \n","    \"validation\":dataset_valid\n","    })"]},{"cell_type":"markdown","metadata":{},"source":["**Limit Dataset Size for Testing:**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:00.175702Z","iopub.status.busy":"2022-09-04T17:20:00.175249Z","iopub.status.idle":"2022-09-04T17:20:00.181198Z","shell.execute_reply":"2022-09-04T17:20:00.180257Z","shell.execute_reply.started":"2022-09-04T17:20:00.175627Z"},"trusted":true},"outputs":[],"source":["## Sample a subset of datapoints\n","#num_samples = 1000\n","#sample_ids = list(range(0,num_samples))\n","#\n","## Reduce the size of the dataset\n","#dataset_train = dataset_train.select(sample_ids)\n","#dataset_valid = dataset_valid.select(sample_ids)\n","#dataset_test = dataset_test.select(sample_ids)\n","#\n","#print(\"Training Examples:\", len(dataset_train))"]},{"cell_type":"markdown","metadata":{},"source":["**Save combined Dataset:**"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["data_path = \"./data/dataset_multilingual.pkl\"\n","with open(data_path, 'wb') as pickle_file:\n","    pickle.dump(obj = dataset, file=pickle_file)"]},{"cell_type":"markdown","metadata":{},"source":["### About the Dataset:"]},{"cell_type":"markdown","metadata":{},"source":["**Splits:**"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:00.183683Z","iopub.status.busy":"2022-09-04T17:20:00.182690Z","iopub.status.idle":"2022-09-04T17:20:00.195729Z","shell.execute_reply":"2022-09-04T17:20:00.194794Z","shell.execute_reply.started":"2022-09-04T17:20:00.183646Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n","        num_rows: 40000\n","    })\n","    test: Dataset({\n","        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n","        num_rows: 20000\n","    })\n","    validation: Dataset({\n","        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n","        num_rows: 20000\n","    })\n","})"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["dataset"]},{"cell_type":"markdown","metadata":{},"source":["**Training Examples:**"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:00.199386Z","iopub.status.busy":"2022-09-04T17:20:00.198536Z","iopub.status.idle":"2022-09-04T17:20:00.205034Z","shell.execute_reply":"2022-09-04T17:20:00.203950Z","shell.execute_reply.started":"2022-09-04T17:20:00.199347Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset Object Type: <class 'datasets.arrow_dataset.Dataset'>\n","Training Examples: 40000\n"]}],"source":["print(\"Dataset Object Type:\", type(dataset[\"train\"]))\n","print(\"Training Examples:\", len(dataset[\"train\"]))"]},{"cell_type":"markdown","metadata":{},"source":["**Sample Structure:**"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:00.207544Z","iopub.status.busy":"2022-09-04T17:20:00.206515Z","iopub.status.idle":"2022-09-04T17:20:00.218353Z","shell.execute_reply":"2022-09-04T17:20:00.217300Z","shell.execute_reply.started":"2022-09-04T17:20:00.207487Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'tokens': ['Bruce', 'Beresford', '(', 'Non-Jew', ')'],\n"," 'ner_tags': [1, 2, 0, 0, 0],\n"," 'langs': ['en', 'en', 'en', 'en', 'en'],\n"," 'spans': ['PER: Bruce Beresford']}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["dataset[\"train\"][95]"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-08-08T16:13:00.916799Z","iopub.status.busy":"2022-08-08T16:13:00.916311Z","iopub.status.idle":"2022-08-08T16:13:00.924905Z","shell.execute_reply":"2022-08-08T16:13:00.922929Z","shell.execute_reply.started":"2022-08-08T16:13:00.916763Z"}},"source":["**Class Labels:**"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:00.222491Z","iopub.status.busy":"2022-09-04T17:20:00.220984Z","iopub.status.idle":"2022-09-04T17:20:00.228875Z","shell.execute_reply":"2022-09-04T17:20:00.227873Z","shell.execute_reply.started":"2022-09-04T17:20:00.222453Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC']\n"]}],"source":["label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n","print(label_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"44f1a40a401e6fa57b592d0b8560dd7ec3f203a86fbbd96af8799189ea4c1abc"}}},"nbformat":4,"nbformat_minor":4}
