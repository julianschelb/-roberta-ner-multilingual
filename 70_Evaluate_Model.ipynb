{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Finetuning RoBERTa for NER: Evaluate Model\n"," "]},{"cell_type":"markdown","metadata":{},"source":["***"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:17:23.428297Z","iopub.status.busy":"2022-09-04T17:17:23.427338Z","iopub.status.idle":"2022-09-04T17:17:31.843100Z","shell.execute_reply":"2022-09-04T17:17:31.842031Z","shell.execute_reply.started":"2022-09-04T17:17:23.427831Z"},"trusted":true},"outputs":[],"source":["from transformers import (BertTokenizerFast,\n","                          RobertaTokenizerFast,\n","                          AutoTokenizer,\n","                          BertForTokenClassification,\n","                          RobertaForTokenClassification,\n","                          DataCollatorForTokenClassification, \n","                          AutoModelForTokenClassification, \n","                          TrainingArguments, Trainer)\n","from datasets import load_dataset, load_metric, concatenate_datasets, DatasetDict\n","from pprint import pprint\n","import numpy as np\n","import pickle\n","import torch\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["data_path = \"./data/dataset_processed.pkl\"\n","with open(data_path, 'rb') as pickle_file:\n","    dataset = pickle.load(file=pickle_file)"]},{"cell_type":"markdown","metadata":{},"source":["## Load Model and Tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["Information about model variants can be found here: https://huggingface.co/docs/transformers/model_doc/roberta\n","\n","Load Model which was finetuned:"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:35.622912Z","iopub.status.busy":"2022-09-04T17:20:35.622133Z","iopub.status.idle":"2022-09-04T17:20:35.820853Z","shell.execute_reply":"2022-09-04T17:20:35.819482Z","shell.execute_reply.started":"2022-09-04T17:20:35.622874Z"},"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:35.823680Z","iopub.status.busy":"2022-09-04T17:20:35.823343Z","iopub.status.idle":"2022-09-04T17:20:35.836229Z","shell.execute_reply":"2022-09-04T17:20:35.832509Z","shell.execute_reply.started":"2022-09-04T17:20:35.823652Z"},"trusted":true},"outputs":[],"source":["label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:17:33.656105Z","iopub.status.busy":"2022-09-04T17:17:33.655459Z","iopub.status.idle":"2022-09-04T17:19:25.869219Z","shell.execute_reply":"2022-09-04T17:19:25.868230Z","shell.execute_reply.started":"2022-09-04T17:17:33.656069Z"},"trusted":true},"outputs":[],"source":["# model_name = \"xlm-roberta-large\" #\"bert-base-multilingual-cased\" #xlm-roberta-large\n","tokenizer = AutoTokenizer.from_pretrained(\"./results/checkpoint-final/\", add_prefix_space=True) #AutoTokenizer(use_fast = True)\n","model = AutoModelForTokenClassification.from_pretrained(\"./results/checkpoint-final/\")"]},{"cell_type":"markdown","metadata":{},"source":["**Define Metrics:**\n","\n","See https://huggingface.co/course/chapter7/2#metrics"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:52.528408Z","iopub.status.busy":"2022-09-04T17:23:52.527563Z","iopub.status.idle":"2022-09-04T17:23:53.180502Z","shell.execute_reply":"2022-09-04T17:23:53.179321Z","shell.execute_reply.started":"2022-09-04T17:23:52.528364Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_1946966/152412463.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n","  metric = load_metric(\"seqeval\")\n"]}],"source":["metric = load_metric(\"seqeval\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'tokens': ['Auch', 'anschlie√üend', 'blieb', 'er', 'der', 'Mannschaft', 'von', 'Trainer', 'Per', 'Olsson', 'treu', ',', 'im', 'Sommer', '2011', 'verl√§ngerte', 'er', 'seinen', 'Kontrakt', 'erneut', 'langfristig', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'langs': ['de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de', 'de'], 'spans': ['PER: Per Olsson'], 'input_ids': [12717, 133177, 178814, 72, 122, 132002, 542, 119205, 908, 9295, 4503, 1360, 34, 6, 4, 566, 29924, 1392, 241960, 13, 72, 25080, 3692, 44962, 119054, 165335, 6, 5], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]}],"source":["print(dataset[\"train\"][150])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:54.086675Z","iopub.status.busy":"2022-09-04T17:23:54.086175Z","iopub.status.idle":"2022-09-04T17:23:54.108136Z","shell.execute_reply":"2022-09-04T17:23:54.107103Z","shell.execute_reply.started":"2022-09-04T17:23:54.086641Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n"," 'overall_precision': 1.0,\n"," 'overall_recall': 1.0,\n"," 'overall_f1': 1.0,\n"," 'overall_accuracy': 1.0}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["example = dataset[\"train\"][150]\n","labels = [label_list[i] for i in example[f\"ner_tags\"]]\n","metric.compute(predictions=[labels], references=[labels])"]},{"cell_type":"markdown","metadata":{},"source":["**Calculate Accuracy:**"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","trainer = Trainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: spans, ner_tags, langs, tokens. If spans, ner_tags, langs, tokens are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 20000\n","  Batch size = 16\n","You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["predictions, labels, _ = trainer.predict(dataset[\"test\"])\n","predictions = np.argmax(predictions, axis=-1)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["label_names = dataset[\"train\"].features[f\"ner_tags\"].feature.names"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'LOC': {'f1': 0.8512989302927002,\n","         'number': 21063,\n","         'precision': 0.843191132637854,\n","         'recall': 0.8595641646489104},\n"," 'ORG': {'f1': 0.7326220690065083,\n","         'number': 16972,\n","         'precision': 0.7391017569107153,\n","         'recall': 0.7262550082488806},\n"," 'PER': {'f1': 0.8671199011124845,\n","         'number': 14649,\n","         'precision': 0.8723316062176166,\n","         'recall': 0.8619701003481466},\n"," 'overall_accuracy': 0.9284292732082127,\n"," 'overall_f1': 0.8177536369506591,\n"," 'overall_precision': 0.8182198236546062,\n"," 'overall_recall': 0.817287981170754}\n"]}],"source":["true_labels = [\n","    [label_names[l] for l in label  if l != -100] \n","    for label in labels\n","]\n","\n","true_predictions = [\n","    [label_names[p] for (p, l) in zip(prediction, label)  if l != -100]\n","    for prediction, label in zip(predictions, labels)\n","]\n","\n","results = metric.compute(predictions=true_predictions, references=true_labels)\n","pprint(results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"transformers","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"c97fe7fd1c286c8419eb1bea19acb7c2170e8f8ba541cc471414a0cdc49a8156"}}},"nbformat":4,"nbformat_minor":4}
