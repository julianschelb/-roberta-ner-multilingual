{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Finetuning RoBERTa for NER: Evaluate Model\n"," "]},{"cell_type":"markdown","metadata":{},"source":["***"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:17:23.428297Z","iopub.status.busy":"2022-09-04T17:17:23.427338Z","iopub.status.idle":"2022-09-04T17:17:31.843100Z","shell.execute_reply":"2022-09-04T17:17:31.842031Z","shell.execute_reply.started":"2022-09-04T17:17:23.427831Z"},"trusted":true},"outputs":[],"source":["from transformers import (BertTokenizerFast,\n","                          RobertaTokenizerFast,\n","                          AutoTokenizer,\n","                          BertForTokenClassification,\n","                          RobertaForTokenClassification,\n","                          DataCollatorForTokenClassification, \n","                          AutoModelForTokenClassification, \n","                          TrainingArguments, Trainer)\n","from datasets import load_dataset, load_metric, concatenate_datasets, DatasetDict\n","from pprint import pprint\n","import numpy as np\n","import pickle\n","import torch\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["data_path = \"./data/dataset_processed.pkl\"\n","with open(data_path, 'rb') as pickle_file:\n","    dataset = pickle.load(file=pickle_file)"]},{"cell_type":"markdown","metadata":{},"source":["## Load Model and Tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["Information about model variants can be found here: https://huggingface.co/docs/transformers/model_doc/roberta\n","\n","Load Model which was finetuned:"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:35.622912Z","iopub.status.busy":"2022-09-04T17:20:35.622133Z","iopub.status.idle":"2022-09-04T17:20:35.820853Z","shell.execute_reply":"2022-09-04T17:20:35.819482Z","shell.execute_reply.started":"2022-09-04T17:20:35.622874Z"},"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:35.823680Z","iopub.status.busy":"2022-09-04T17:20:35.823343Z","iopub.status.idle":"2022-09-04T17:20:35.836229Z","shell.execute_reply":"2022-09-04T17:20:35.832509Z","shell.execute_reply.started":"2022-09-04T17:20:35.823652Z"},"trusted":true},"outputs":[],"source":["label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:17:33.656105Z","iopub.status.busy":"2022-09-04T17:17:33.655459Z","iopub.status.idle":"2022-09-04T17:19:25.869219Z","shell.execute_reply":"2022-09-04T17:19:25.868230Z","shell.execute_reply.started":"2022-09-04T17:17:33.656069Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["loading file sentencepiece.bpe.model\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n","loading configuration file ./results/checkpoint-final/config.json\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"./results/checkpoint-final/\",\n","  \"architectures\": [\n","    \"XLMRobertaForTokenClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"O\",\n","    \"1\": \"B-PER\",\n","    \"2\": \"I-PER\",\n","    \"3\": \"B-ORG\",\n","    \"4\": \"I-ORG\",\n","    \"5\": \"B-LOC\",\n","    \"6\": \"I-LOC\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"label2id\": {\n","    \"B-LOC\": 5,\n","    \"B-ORG\": 3,\n","    \"B-PER\": 1,\n","    \"I-LOC\": 6,\n","    \"I-ORG\": 4,\n","    \"I-PER\": 2,\n","    \"O\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.25.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file ./results/checkpoint-final/pytorch_model.bin\n","All model checkpoint weights were used when initializing XLMRobertaForTokenClassification.\n","\n","All the weights of XLMRobertaForTokenClassification were initialized from the model checkpoint at ./results/checkpoint-final/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForTokenClassification for predictions without further training.\n"]}],"source":["# model_name = \"xlm-roberta-large\" #\"bert-base-multilingual-cased\" #xlm-roberta-large\n","tokenizer = AutoTokenizer.from_pretrained(\"./results/checkpoint-final/\", add_prefix_space=True) #AutoTokenizer(use_fast = True)\n","model = AutoModelForTokenClassification.from_pretrained(\"./results/checkpoint-final/\")"]},{"cell_type":"markdown","metadata":{},"source":["**Define Metrics:**\n","\n","See https://huggingface.co/course/chapter7/2#metrics"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:52.528408Z","iopub.status.busy":"2022-09-04T17:23:52.527563Z","iopub.status.idle":"2022-09-04T17:23:53.180502Z","shell.execute_reply":"2022-09-04T17:23:53.179321Z","shell.execute_reply.started":"2022-09-04T17:23:52.528364Z"},"trusted":true},"outputs":[],"source":["metric = load_metric(\"seqeval\")"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'tokens': [\"''\", 'Psolos', 'fuligo', 'fuligo', \"''\", 'â€”', \"''\", \"'Coon\", \"''\", \"'\"], 'ner_tags': [0, 5, 6, 6, 0, 0, 0, 0, 0, 0], 'langs': ['en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en'], 'spans': ['LOC: Psolos fuligo fuligo'], 'input_ids': [5106, 436, 112032, 7, 45201, 20828, 45201, 20828, 5106, 292, 5106, 242, 10625, 191, 5106, 242], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [0, 5, 5, 5, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0]}\n"]}],"source":["print(dataset[\"train\"][150])"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:54.086675Z","iopub.status.busy":"2022-09-04T17:23:54.086175Z","iopub.status.idle":"2022-09-04T17:23:54.108136Z","shell.execute_reply":"2022-09-04T17:23:54.107103Z","shell.execute_reply.started":"2022-09-04T17:23:54.086641Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n"," 'overall_precision': 1.0,\n"," 'overall_recall': 1.0,\n"," 'overall_f1': 1.0,\n"," 'overall_accuracy': 1.0}"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["example = dataset[\"train\"][150]\n","labels = [label_list[i] for i in example[f\"ner_tags\"]]\n","metric.compute(predictions=[labels], references=[labels])"]},{"cell_type":"markdown","metadata":{},"source":["**Calculate Accuracy:**"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}],"source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n","trainer = Trainer(\n","    model=model,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n",")"]},{"cell_type":"code","execution_count":38,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set don't have a corresponding argument in `XLMRobertaForTokenClassification.forward` and have been ignored: tokens, spans, langs, ner_tags. If tokens, spans, langs, ner_tags are not expected by `XLMRobertaForTokenClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1000\n","  Batch size = 16\n","You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["predictions, labels, _ = trainer.predict(dataset[\"test\"])\n","predictions = np.argmax(predictions, axis=-1)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["label_names = dataset[\"train\"].features[f\"ner_tags\"].feature.names"]},{"cell_type":"code","execution_count":40,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'LOC': {'f1': 0.5492468134414833,\n","         'number': 880,\n","         'precision': 0.5602836879432624,\n","         'recall': 0.5386363636363637},\n"," 'ORG': {'f1': 0.40428211586901763,\n","         'number': 758,\n","         'precision': 0.38674698795180723,\n","         'recall': 0.4234828496042216},\n"," 'PER': {'f1': 0.6785508913168488,\n","         'number': 782,\n","         'precision': 0.6165099268547545,\n","         'recall': 0.7544757033248082},\n"," 'overall_accuracy': 0.8205149229255494,\n"," 'overall_f1': 0.5481891945378983,\n"," 'overall_precision': 0.5260159513862515,\n"," 'overall_recall': 0.5723140495867769}\n"]}],"source":["true_labels = [\n","    [label_names[l] for l in label  if l != -100] \n","    for label in labels\n","]\n","\n","true_predictions = [\n","    [label_names[p] for (p, l) in zip(prediction, label)  if l != -100]\n","    for prediction, label in zip(predictions, labels)\n","]\n","\n","results = metric.compute(predictions=true_predictions, references=true_labels)\n","pprint(results)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"transformers","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"c97fe7fd1c286c8419eb1bea19acb7c2170e8f8ba541cc471414a0cdc49a8156"}}},"nbformat":4,"nbformat_minor":4}
