{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning RoBERTa for NER: Train Model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:17:23.428297Z",
     "iopub.status.busy": "2022-09-04T17:17:23.427338Z",
     "iopub.status.idle": "2022-09-04T17:17:31.843100Z",
     "shell.execute_reply": "2022-09-04T17:17:31.842031Z",
     "shell.execute_reply.started": "2022-09-04T17:17:23.427831Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (BertTokenizerFast,\n",
    "                          RobertaTokenizerFast,\n",
    "                          AutoTokenizer,\n",
    "                          BertForTokenClassification,\n",
    "                          RobertaForTokenClassification,\n",
    "                          DataCollatorForTokenClassification, \n",
    "                          AutoModelForTokenClassification, \n",
    "                          TrainingArguments, Trainer)\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets, DatasetDict\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "import torch\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/dataset_processed.pkl\"\n",
    "with open(data_path, 'rb') as pickle_file:\n",
    "    dataset = pickle.load(file=pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [\"''\", 'Super', 'World', 'of', 'Sports', \"''\", \"'\"],\n",
       " 'ner_tags': [0, 3, 4, 4, 4, 0, 0],\n",
       " 'langs': ['en', 'en', 'en', 'en', 'en', 'en', 'en'],\n",
       " 'spans': ['ORG: Super World of Sports'],\n",
       " 'input_ids': [5106, 4265, 6661, 111, 39170, 5106, 242],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [0, 3, 4, 4, 4, 0, 0]}"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about model variants can be found here: https://huggingface.co/docs/transformers/model_doc/roberta\n",
    "\n",
    "Load Model which can be finetuned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:20:35.622912Z",
     "iopub.status.busy": "2022-09-04T17:20:35.622133Z",
     "iopub.status.idle": "2022-09-04T17:20:35.820853Z",
     "shell.execute_reply": "2022-09-04T17:20:35.819482Z",
     "shell.execute_reply.started": "2022-09-04T17:20:35.622874Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:20:35.823680Z",
     "iopub.status.busy": "2022-09-04T17:20:35.823343Z",
     "iopub.status.idle": "2022-09-04T17:20:35.836229Z",
     "shell.execute_reply": "2022-09-04T17:20:35.832509Z",
     "shell.execute_reply.started": "2022-09-04T17:20:35.823652Z"
    }
   },
   "outputs": [],
   "source": [
    "label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:17:33.656105Z",
     "iopub.status.busy": "2022-09-04T17:17:33.655459Z",
     "iopub.status.idle": "2022-09-04T17:19:25.869219Z",
     "shell.execute_reply": "2022-09-04T17:19:25.868230Z",
     "shell.execute_reply.started": "2022-09-04T17:17:33.656069Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at /home/pop529700/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading file sentencepiece.bpe.model from cache at /home/pop529700/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /home/pop529700/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading configuration file config.json from cache at /home/pop529700/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/pop529700/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/pop529700/.cache/huggingface/hub/models--xlm-roberta-large/snapshots/b2a6150f8be56457baf80c74342cc424080260f0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"xlm-roberta-large\" #\"bert-base-multilingual-cased\" #xlm-roberta-large\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{model_name}\", add_prefix_space=True) #AutoTokenizer(use_fast = True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(f\"{model_name}\", num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:20:35.615127Z",
     "iopub.status.busy": "2022-09-04T17:20:35.614018Z",
     "iopub.status.idle": "2022-09-04T17:20:35.620697Z",
     "shell.execute_reply": "2022-09-04T17:20:35.619590Z",
     "shell.execute_reply.started": "2022-09-04T17:20:35.615098Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the Model:\n",
    "\n",
    "see https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/modeling_utils.py#L829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:20:49.161724Z",
     "iopub.status.busy": "2022-09-04T17:20:49.161376Z",
     "iopub.status.idle": "2022-09-04T17:20:49.174161Z",
     "shell.execute_reply": "2022-09-04T17:20:49.173299Z",
     "shell.execute_reply.started": "2022-09-04T17:20:49.161688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 558848007\n",
      "Expected Input Dict: input_ids\n",
      "FLOPS needed per Training Sample: 23471616294\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameters:\", model.num_parameters())\n",
    "print(\"Expected Input Dict:\", model.main_input_name )\n",
    "\n",
    "# Estimate FLOPS needed for one training example\n",
    "sample = dataset[\"train\"][0]\n",
    "sample[\"input_ids\"] = torch.Tensor(sample[\"input_ids\"])\n",
    "flops_est = model.floating_point_ops(input_dict = sample, exclude_embeddings = False)\n",
    "\n",
    "print(\"FLOPS needed per Training Sample:\", flops_est )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'spans', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'spans', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'spans', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Optimizer:**\n",
    "\n",
    "See https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.Adafactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:20:49.176350Z",
     "iopub.status.busy": "2022-09-04T17:20:49.175774Z",
     "iopub.status.idle": "2022-09-04T17:20:49.181946Z",
     "shell.execute_reply": "2022-09-04T17:20:49.180928Z",
     "shell.execute_reply.started": "2022-09-04T17:20:49.176314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 156.25\n"
     ]
    }
   ],
   "source": [
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "num_reports = 5\n",
    "\n",
    "# A training step is one gradient update. In one step batch_size examples are processed.\n",
    "# An epoch consists of one full cycle through the training data. \n",
    "# This is usually many steps. As an example, if you have 2,000 images and use\n",
    "# a batch size of 10 an epoch consists of:\n",
    "gpu_count = torch.cuda.device_count()\n",
    "num_steps = (len(dataset[\"train\"]) / batch_size / gpu_count) * num_epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-6, weight_decay=0.01, no_deprecation_warning= True)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps= num_steps \n",
    ")\n",
    "\n",
    "print(\"Steps:\", num_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Log and Eval Interval:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval interval: 31\n"
     ]
    }
   ],
   "source": [
    "report_steps = math.floor(num_steps / num_reports)\n",
    "print(\"Eval interval:\", report_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Metrics:**\n",
    "\n",
    "See https://huggingface.co/course/chapter7/2#metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:52.528408Z",
     "iopub.status.busy": "2022-09-04T17:23:52.527563Z",
     "iopub.status.idle": "2022-09-04T17:23:53.180502Z",
     "shell.execute_reply": "2022-09-04T17:23:53.179321Z",
     "shell.execute_reply.started": "2022-09-04T17:23:52.528364Z"
    }
   },
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:54.086675Z",
     "iopub.status.busy": "2022-09-04T17:23:54.086175Z",
     "iopub.status.idle": "2022-09-04T17:23:54.108136Z",
     "shell.execute_reply": "2022-09-04T17:23:54.107103Z",
     "shell.execute_reply.started": "2022-09-04T17:23:54.086641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 3},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"train\"][150]\n",
    "labels = [label_list[i] for i in example[f\"labels\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set correct class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:54.949476Z",
     "iopub.status.busy": "2022-09-04T17:23:54.949090Z",
     "iopub.status.idle": "2022-09-04T17:23:54.956761Z",
     "shell.execute_reply": "2022-09-04T17:23:54.955061Z",
     "shell.execute_reply.started": "2022-09-04T17:23:54.949441Z"
    }
   },
   "outputs": [],
   "source": [
    "label_names = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "\n",
    "id2label = {id : label for id, label in enumerate(label_names)}\n",
    "label2id = {label: id for id, label in enumerate(label_names)}\n",
    "\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:55.633459Z",
     "iopub.status.busy": "2022-09-04T17:23:55.633071Z",
     "iopub.status.idle": "2022-09-04T17:23:55.639657Z",
     "shell.execute_reply": "2022-09-04T17:23:55.638357Z",
     "shell.execute_reply.started": "2022-09-04T17:23:55.633426Z"
    }
   },
   "source": [
    "Define callback function to evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:55.991541Z",
     "iopub.status.busy": "2022-09-04T17:23:55.990958Z",
     "iopub.status.idle": "2022-09-04T17:23:56.000348Z",
     "shell.execute_reply": "2022-09-04T17:23:55.998152Z",
     "shell.execute_reply.started": "2022-09-04T17:23:55.991502Z"
    }
   },
   "outputs": [],
   "source": [
    "label_names = model.config.id2label\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = [[label_names[l] for l in label  if l != -100] for label in labels]\n",
    "    #true_predictions = [model.config.id2label[t.item()] for t in predictions]\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label)  if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove unnecessary columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"tokens\", \"ner_tags\", \"langs\", \"spans\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set further Training Arguments:**\n",
    "\n",
    "See https://huggingface.co/docs/transformers/v4.21.2/en/main_classes/trainer#transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:56.904243Z",
     "iopub.status.busy": "2022-09-04T17:23:56.903622Z",
     "iopub.status.idle": "2022-09-04T17:23:56.925735Z",
     "shell.execute_reply": "2022-09-04T17:23:56.924817Z",
     "shell.execute_reply.started": "2022-09-04T17:23:56.904204Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = report_steps,\n",
    "    remove_unused_columns = True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = report_steps,\n",
    "    #load_best_model_at_end=True,\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = report_steps,\n",
    "    #learning_rate= 2e-5,\n",
    "    #auto_find_batch_size = True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    #optim=\"adamw_torch\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    #weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "GPU used by Kaggle: https://www.nvidia.com/de-de/data-center/tesla-p100/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:58.843624Z",
     "iopub.status.busy": "2022-09-04T17:23:58.842692Z",
     "iopub.status.idle": "2022-09-04T17:23:59.968744Z",
     "shell.execute_reply": "2022-09-04T17:23:59.967514Z",
     "shell.execute_reply.started": "2022-09-04T17:23:58.843577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan  7 13:07:22 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          Off  | 00000000:01:00.0 Off |                    0 |\n",
      "|  0%   48C    P0    81W / 300W |  34810MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A40          Off  | 00000000:43:00.0 Off |                    0 |\n",
      "|  0%   49C    P0    82W / 300W |   4852MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1549808      G   /usr/lib/xorg/Xorg                 23MiB |\n",
      "|    0   N/A  N/A   1860009      C   ...s/transformers/bin/python    29359MiB |\n",
      "|    0   N/A  N/A   1874130      C   ...s/transformers/bin/python     5425MiB |\n",
      "|    1   N/A  N/A   1549808      G   /usr/lib/xorg/Xorg                 23MiB |\n",
      "|    1   N/A  N/A   1860009      C   ...s/transformers/bin/python     1197MiB |\n",
      "|    1   N/A  N/A   1874130      C   ...s/transformers/bin/python     3629MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:59.972792Z",
     "iopub.status.busy": "2022-09-04T17:23:59.971064Z",
     "iopub.status.idle": "2022-09-04T17:40:47.605893Z",
     "shell.execute_reply": "2022-09-04T17:40:47.604419Z",
     "shell.execute_reply.started": "2022-09-04T17:23:59.972745Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 160\n",
      "  Number of trainable parameters = 558848007\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 01:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.788100</td>\n",
       "      <td>1.475708</td>\n",
       "      <td>0.067762</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>0.038892</td>\n",
       "      <td>0.478928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1.382300</td>\n",
       "      <td>1.148932</td>\n",
       "      <td>0.144931</td>\n",
       "      <td>0.235124</td>\n",
       "      <td>0.179326</td>\n",
       "      <td>0.605034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>1.077200</td>\n",
       "      <td>0.880566</td>\n",
       "      <td>0.277396</td>\n",
       "      <td>0.316942</td>\n",
       "      <td>0.295853</td>\n",
       "      <td>0.696704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.881500</td>\n",
       "      <td>0.787374</td>\n",
       "      <td>0.341313</td>\n",
       "      <td>0.365289</td>\n",
       "      <td>0.352894</td>\n",
       "      <td>0.737783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.809300</td>\n",
       "      <td>0.722780</td>\n",
       "      <td>0.379388</td>\n",
       "      <td>0.419835</td>\n",
       "      <td>0.398588</td>\n",
       "      <td>0.759347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-31\n",
      "Configuration saved in ./results/checkpoint-31/config.json\n",
      "Model weights saved in ./results/checkpoint-31/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-31/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-31/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-62\n",
      "Configuration saved in ./results/checkpoint-62/config.json\n",
      "Model weights saved in ./results/checkpoint-62/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-62/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-62/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-93\n",
      "Configuration saved in ./results/checkpoint-93/config.json\n",
      "Model weights saved in ./results/checkpoint-93/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-93/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-93/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-124\n",
      "Configuration saved in ./results/checkpoint-124/config.json\n",
      "Model weights saved in ./results/checkpoint-124/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-124/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-124/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-155\n",
      "Configuration saved in ./results/checkpoint-155/config.json\n",
      "Model weights saved in ./results/checkpoint-155/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-155/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-155/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=1.174519120156765, metrics={'train_runtime': 118.0347, 'train_samples_per_second': 42.36, 'train_steps_per_second': 1.356, 'total_flos': 387727231040688.0, 'train_loss': 1.174519120156765, 'epoch': 5.0})"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.7214924097061157\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Eval Loss: {eval_results['eval_loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the fine tuned model & tokenizer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-final/\n",
      "Configuration saved in ./results/checkpoint-final/config.json\n",
      "Model weights saved in ./results/checkpoint-final/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-final/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-final/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(f'./results/checkpoint-final/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Training History:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./results/checkpoint-final/training_args.pkl\"\n",
    "with open(data_path, 'wb') as pickle_file:\n",
    "    pickle.dump(obj = trainer.args, file=pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./results/checkpoint-final/training_history.pkl\"\n",
    "with open(data_path, 'wb') as pickle_file:\n",
    "    pickle.dump(obj = trainer.state, file=pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Accuracy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(dataset[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'f1': 0.42008691453404157,\n",
      "         'number': 880,\n",
      "         'precision': 0.36523929471032746,\n",
      "         'recall': 0.4943181818181818},\n",
      " 'ORG': {'f1': 0.24005681818181818,\n",
      "         'number': 758,\n",
      "         'precision': 0.26,\n",
      "         'recall': 0.22295514511873352},\n",
      " 'PER': {'f1': 0.5234228607120549,\n",
      "         'number': 782,\n",
      "         'precision': 0.5115995115995116,\n",
      "         'recall': 0.5358056265984654},\n",
      " 'overall_accuracy': 0.7610692030173828,\n",
      " 'overall_f1': 0.402755905511811,\n",
      " 'overall_precision': 0.38458646616541353,\n",
      " 'overall_recall': 0.42272727272727273}\n"
     ]
    }
   ],
   "source": [
    "true_labels = [\n",
    "    [label_names[l] for l in label  if l != -100] \n",
    "    for label in labels\n",
    "]\n",
    "\n",
    "true_predictions = [\n",
    "    [label_names[p] for (p, l) in zip(prediction, label)  if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c97fe7fd1c286c8419eb1bea19acb7c2170e8f8ba541cc471414a0cdc49a8156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
