{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Finetuning RoBERTa for NER: Train Model\n"," "]},{"cell_type":"markdown","metadata":{},"source":["***"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:17:23.428297Z","iopub.status.busy":"2022-09-04T17:17:23.427338Z","iopub.status.idle":"2022-09-04T17:17:31.843100Z","shell.execute_reply":"2022-09-04T17:17:31.842031Z","shell.execute_reply.started":"2022-09-04T17:17:23.427831Z"},"trusted":true},"outputs":[],"source":["from transformers import (BertTokenizerFast,\n","                          RobertaTokenizerFast,\n","                          AutoTokenizer,\n","                          BertForTokenClassification,\n","                          RobertaForTokenClassification,\n","                          DataCollatorForTokenClassification, \n","                          AutoModelForTokenClassification, \n","                          TrainingArguments, Trainer)\n","from datasets import load_dataset, load_metric, concatenate_datasets, DatasetDict\n","from pprint import pprint\n","import numpy as np\n","import pickle\n","import torch\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["data_path = \"./data/dataset_processed.pkl\"\n","with open(data_path, 'rb') as pickle_file:\n","    dataset = pickle.load(file=pickle_file)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["{'tokens': ['Ashvin', 'Raja', 'as', 'Paalpandi'],\n"," 'ner_tags': [1, 2, 0, 0],\n"," 'langs': ['en', 'en', 'en', 'en'],\n"," 'spans': ['PER: Ashvin Raja'],\n"," 'input_ids': [39055, 2082, 19270, 237, 1342, 289, 159945],\n"," 'attention_mask': [1, 1, 1, 1, 1, 1, 1],\n"," 'labels': [1, 1, 2, 0, 0, 0, 0]}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["dataset[\"train\"][0]"]},{"cell_type":"markdown","metadata":{},"source":["## Load Model and Tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["Information about model variants can be found here: https://huggingface.co/docs/transformers/model_doc/roberta\n","\n","Load Model which can be finetuned:"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:35.622912Z","iopub.status.busy":"2022-09-04T17:20:35.622133Z","iopub.status.idle":"2022-09-04T17:20:35.820853Z","shell.execute_reply":"2022-09-04T17:20:35.819482Z","shell.execute_reply.started":"2022-09-04T17:20:35.622874Z"},"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:35.823680Z","iopub.status.busy":"2022-09-04T17:20:35.823343Z","iopub.status.idle":"2022-09-04T17:20:35.836229Z","shell.execute_reply":"2022-09-04T17:20:35.832509Z","shell.execute_reply.started":"2022-09-04T17:20:35.823652Z"},"trusted":true},"outputs":[],"source":["label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:17:33.656105Z","iopub.status.busy":"2022-09-04T17:17:33.655459Z","iopub.status.idle":"2022-09-04T17:19:25.869219Z","shell.execute_reply":"2022-09-04T17:19:25.868230Z","shell.execute_reply.started":"2022-09-04T17:17:33.656069Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["model_name = \"xlm-roberta-large\" #\"bert-base-multilingual-cased\" #xlm-roberta-large\n","tokenizer = AutoTokenizer.from_pretrained(f\"{model_name}\", add_prefix_space=True) #AutoTokenizer(use_fast = True)\n","model = AutoModelForTokenClassification.from_pretrained(f\"{model_name}\", num_labels=len(label_list))"]},{"cell_type":"markdown","metadata":{},"source":["## Define Data Collator"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:35.615127Z","iopub.status.busy":"2022-09-04T17:20:35.614018Z","iopub.status.idle":"2022-09-04T17:20:35.620697Z","shell.execute_reply":"2022-09-04T17:20:35.619590Z","shell.execute_reply.started":"2022-09-04T17:20:35.615098Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["## Define Trainer"]},{"cell_type":"markdown","metadata":{},"source":["About the Model:\n","\n","see https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/modeling_utils.py#L829"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:49.161724Z","iopub.status.busy":"2022-09-04T17:20:49.161376Z","iopub.status.idle":"2022-09-04T17:20:49.174161Z","shell.execute_reply":"2022-09-04T17:20:49.173299Z","shell.execute_reply.started":"2022-09-04T17:20:49.161688Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Parameters: 558842882\n","Expected Input Dict: input_ids\n","FLOPS needed per Training Sample: 23471401044\n"]}],"source":["print(\"Parameters:\", model.num_parameters())\n","print(\"Expected Input Dict:\", model.main_input_name )\n","\n","# Estimate FLOPS needed for one training example\n","sample = dataset[\"train\"][0]\n","sample[\"input_ids\"] = torch.Tensor(sample[\"input_ids\"])\n","flops_est = model.floating_point_ops(input_dict = sample, exclude_embeddings = False)\n","\n","print(\"FLOPS needed per Training Sample:\", flops_est )"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['tokens', 'ner_tags', 'langs', 'spans', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 40000\n","    })\n","    test: Dataset({\n","        features: ['tokens', 'ner_tags', 'langs', 'spans', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 20000\n","    })\n","    validation: Dataset({\n","        features: ['tokens', 'ner_tags', 'langs', 'spans', 'input_ids', 'attention_mask', 'labels'],\n","        num_rows: 20000\n","    })\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["dataset\n"]},{"cell_type":"markdown","metadata":{},"source":["**Define Optimizer:**\n","\n","See https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.Adafactor"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:49.176350Z","iopub.status.busy":"2022-09-04T17:20:49.175774Z","iopub.status.idle":"2022-09-04T17:20:49.181946Z","shell.execute_reply":"2022-09-04T17:20:49.180928Z","shell.execute_reply.started":"2022-09-04T17:20:49.176314Z"},"trusted":true},"outputs":[],"source":["#from transformers.optimization import Adafactor, AdafactorSchedule\n","\n","#optimizer = Adafactor(    \n","#        model.parameters(),\n","#        lr=1e-3,\n","#        eps=(1e-30, 1e-3),\n","#        clip_threshold=1.0,\n","#        decay_rate=-0.8,\n","#        beta1=None,\n","#        weight_decay=0.0,\n","#        relative_step=False,\n","#        scale_parameter=False,\n","#        warmup_init=False,\n","#    )\n","\n","#lr_scheduler = AdafactorSchedule(optimizer)"]},{"cell_type":"markdown","metadata":{},"source":["**Define Metrics:**\n","\n","See https://huggingface.co/course/chapter7/2#metrics"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:52.528408Z","iopub.status.busy":"2022-09-04T17:23:52.527563Z","iopub.status.idle":"2022-09-04T17:23:53.180502Z","shell.execute_reply":"2022-09-04T17:23:53.179321Z","shell.execute_reply.started":"2022-09-04T17:23:52.528364Z"},"trusted":true},"outputs":[],"source":["metric = load_metric(\"seqeval\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:54.086675Z","iopub.status.busy":"2022-09-04T17:23:54.086175Z","iopub.status.idle":"2022-09-04T17:23:54.108136Z","shell.execute_reply":"2022-09-04T17:23:54.107103Z","shell.execute_reply.started":"2022-09-04T17:23:54.086641Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n"," 'overall_precision': 1.0,\n"," 'overall_recall': 1.0,\n"," 'overall_f1': 1.0,\n"," 'overall_accuracy': 1.0}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["example = dataset[\"train\"][150]\n","labels = [label_list[i] for i in example[f\"labels\"]]\n","metric.compute(predictions=[labels], references=[labels])"]},{"cell_type":"markdown","metadata":{},"source":["Set correct class labels:"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:54.949476Z","iopub.status.busy":"2022-09-04T17:23:54.949090Z","iopub.status.idle":"2022-09-04T17:23:54.956761Z","shell.execute_reply":"2022-09-04T17:23:54.955061Z","shell.execute_reply.started":"2022-09-04T17:23:54.949441Z"},"trusted":true},"outputs":[],"source":["label_names = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n","\n","id2label = {id : label for id, label in enumerate(label_names)}\n","label2id = {label: id for id, label in enumerate(label_names)}\n","\n","model.config.id2label = id2label\n","model.config.label2id = label2id"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:55.633459Z","iopub.status.busy":"2022-09-04T17:23:55.633071Z","iopub.status.idle":"2022-09-04T17:23:55.639657Z","shell.execute_reply":"2022-09-04T17:23:55.638357Z","shell.execute_reply.started":"2022-09-04T17:23:55.633426Z"},"trusted":true},"source":["Define callback function to evaluate the model:"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:55.991541Z","iopub.status.busy":"2022-09-04T17:23:55.990958Z","iopub.status.idle":"2022-09-04T17:23:56.000348Z","shell.execute_reply":"2022-09-04T17:23:55.998152Z","shell.execute_reply.started":"2022-09-04T17:23:55.991502Z"},"trusted":true},"outputs":[],"source":["label_names = model.config.id2label\n","\n","def compute_metrics(eval_preds):\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","\n","    true_labels = [[label_names[l] for l in label  if l != -100] for label in labels]\n","    #true_predictions = [model.config.id2label[t.item()] for t in predictions]\n","    \n","    true_predictions = [\n","        [label_names[p] for (p, l) in zip(prediction, label)  if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["**Remove unnecessary columns:**"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["dataset = dataset.remove_columns([\"tokens\", \"ner_tags\", \"langs\", \"spans\"])"]},{"cell_type":"markdown","metadata":{},"source":["**Set further Training Arguments:**\n","\n","See https://huggingface.co/docs/transformers/v4.21.2/en/main_classes/trainer#transformers.TrainingArguments"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:56.904243Z","iopub.status.busy":"2022-09-04T17:23:56.903622Z","iopub.status.idle":"2022-09-04T17:23:56.925735Z","shell.execute_reply":"2022-09-04T17:23:56.924817Z","shell.execute_reply.started":"2022-09-04T17:23:56.904204Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    save_strategy= \"no\",# \"epoch\",\n","    #save_steps = 2000,\n","    remove_unused_columns = True,\n","    evaluation_strategy=\"steps\",\n","    eval_steps = 2000,\n","    #load_best_model_at_end=True,\n","    logging_strategy = \"steps\",\n","    logging_steps = 2000,\n","    learning_rate=2e-5,\n","    #auto_find_batch_size = True,\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    #optim=\"adamw_torch\",\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    report_to=\"none\",\n","    #fp16=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    #optimizers=(optimizer, lr_scheduler),\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Train Model\n","\n","GPU used by Kaggle: https://www.nvidia.com/de-de/data-center/tesla-p100/"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:58.843624Z","iopub.status.busy":"2022-09-04T17:23:58.842692Z","iopub.status.idle":"2022-09-04T17:23:59.968744Z","shell.execute_reply":"2022-09-04T17:23:59.967514Z","shell.execute_reply.started":"2022-09-04T17:23:58.843577Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["'nvidia-smi' is not recognized as an internal or external command,\n","operable program or batch file.\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:59.972792Z","iopub.status.busy":"2022-09-04T17:23:59.971064Z","iopub.status.idle":"2022-09-04T17:40:47.605893Z","shell.execute_reply":"2022-09-04T17:40:47.604419Z","shell.execute_reply.started":"2022-09-04T17:23:59.972745Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\Projects\\roberta-ner-multilingual\\venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 40000\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 1\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 4\n","  Total optimization steps = 50000\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"77e89dc4043b4a108167682da5b20338","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/50000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"IndexError","evalue":"Target 2 is out of bounds.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn [19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Projects\\roberta-ner-multilingual\\venv\\lib\\site-packages\\transformers\\trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1495\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1497\u001b[0m )\n\u001b[1;32m-> 1498\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1499\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1500\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1501\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1502\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1503\u001b[0m )\n","File \u001b[1;32md:\\Projects\\roberta-ner-multilingual\\venv\\lib\\site-packages\\transformers\\trainer.py:1740\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1738\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1739\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1743\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1744\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1745\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1746\u001b[0m ):\n\u001b[0;32m   1747\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n","File \u001b[1;32md:\\Projects\\roberta-ner-multilingual\\venv\\lib\\site-packages\\transformers\\trainer.py:2470\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2467\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2469\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2470\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2472\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2473\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[1;32md:\\Projects\\roberta-ner-multilingual\\venv\\lib\\site-packages\\transformers\\trainer.py:2502\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2501\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2502\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2503\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2504\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n","File \u001b[1;32md:\\Projects\\roberta-ner-multilingual\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\Projects\\roberta-ner-multilingual\\venv\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1421\u001b[0m, in \u001b[0;36mRobertaForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1419\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1420\u001b[0m     loss_fct \u001b[39m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m-> 1421\u001b[0m     loss \u001b[39m=\u001b[39m loss_fct(logits\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_labels), labels\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m   1423\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1424\u001b[0m     output \u001b[39m=\u001b[39m (logits,) \u001b[39m+\u001b[39m outputs[\u001b[39m2\u001b[39m:]\n","File \u001b[1;32md:\\Projects\\roberta-ner-multilingual\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\Projects\\roberta-ner-multilingual\\venv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1164\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1164\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1165\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1166\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n","File \u001b[1;32md:\\Projects\\roberta-ner-multilingual\\venv\\lib\\site-packages\\torch\\nn\\functional.py:3014\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3012\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3013\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3014\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n","\u001b[1;31mIndexError\u001b[0m: Target 2 is out of bounds."]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["eval_results = trainer.evaluate()\n","print(f\"Eval Loss: {eval_results['eval_loss']}\")"]},{"cell_type":"markdown","metadata":{},"source":["**Saving the fine tuned model & tokenizer:**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.save_model(f'./results/checkpoint-final/')"]},{"cell_type":"markdown","metadata":{},"source":["**Calculate Accuracy:**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions, labels, _ = trainer.predict(dataset[\"test\"])\n","predictions = np.argmax(predictions, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["true_labels = [\n","    [label_names[l] for l in label  if l != -100] \n","    for label in labels\n","]\n","\n","true_predictions = [\n","    [label_names[p] for (p, l) in zip(prediction, label)  if l != -100]\n","    for prediction, label in zip(predictions, labels)\n","]\n","\n","results = metric.compute(predictions=true_predictions, references=true_labels)\n","pprint(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training_history = trainer.state.log_history\n","\n","# Training\n","epochs = [epoch.get(\"epoch\") for epoch in  training_history if epoch.get(\"loss\") is not None]\n","steps = [epoch.get(\"step\") for epoch in  training_history if epoch.get(\"loss\") is not None]\n","loss = [epoch.get(\"loss\") for epoch in  training_history if epoch.get(\"loss\") is not None]\n","\n","# Eval\n","eval_epochs = [epoch.get(\"epoch\") for epoch in  training_history if epoch.get(\"eval_loss\") is not None]\n","eval_steps = [epoch.get(\"step\") for epoch in  training_history if epoch.get(\"eval_loss\") is not None]\n","eval_loss = [epoch.get(\"eval_loss\") for epoch in  training_history if epoch.get(\"eval_loss\") is not None]\n","eval_precision = [epoch.get(\"eval_precision\") for epoch in  training_history if epoch.get(\"eval_precision\") is not None]\n","eval_recall = [epoch.get(\"eval_recall\") for epoch in  training_history if epoch.get(\"eval_recall\") is not None]\n","eval_f1 = [epoch.get(\"eval_recall\") for epoch in  training_history if epoch.get(\"eval_f1\") is not None]\n","eval_accuracy = [epoch.get(\"eval_accuracy\") for epoch in  training_history if epoch.get(\"eval_accuracy\") is not None]\n","\n","eval_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["p = sns.lineplot( x=steps, y=loss)\n","p.set_xlabel(\"Training Steps\")\n","p.set_ylabel(\"Loss\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["p = sns.lineplot(x=eval_steps, y=eval_loss)\n","p.set_xlabel(\"Eval Steps\")\n","p.set_ylabel(\"Loss\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["p = sns.lineplot(x=eval_steps, y=eval_accuracy)\n","p.set_xlabel(\"Eval Steps\")\n","p.set_ylabel(\"Loss\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"44f1a40a401e6fa57b592d0b8560dd7ec3f203a86fbbd96af8799189ea4c1abc"}}},"nbformat":4,"nbformat_minor":4}
