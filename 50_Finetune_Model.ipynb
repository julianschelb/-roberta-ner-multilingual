{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Finetuning BLOOM for NER: Train Model\n"," "]},{"cell_type":"markdown","metadata":{},"source":["***"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:17:23.428297Z","iopub.status.busy":"2022-09-04T17:17:23.427338Z","iopub.status.idle":"2022-09-04T17:17:31.843100Z","shell.execute_reply":"2022-09-04T17:17:31.842031Z","shell.execute_reply.started":"2022-09-04T17:17:23.427831Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/julian/Documents/Workspace_Python/bloom-ner-multilingual/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from transformers import (BloomTokenizerFast,\n","                          BloomForTokenClassification,\n","                          DataCollatorForTokenClassification, \n","                          AutoModelForTokenClassification, \n","                          TrainingArguments, Trainer)\n","from datasets import load_dataset, load_metric, concatenate_datasets, DatasetDict\n","from pprint import pprint\n","import numpy as np\n","import pickle\n","import torch\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["data_path = \"./data/dataset_processed.pkl\"\n","with open(data_path, 'rb') as pickle_file:\n","    dataset = pickle.load(file=pickle_file)"]},{"cell_type":"markdown","metadata":{},"source":["## Load Model and Tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["The list of available Models can be found here: https://huggingface.co/docs/transformers/model_doc/bloom\n","\n","Load Model which can be finetuned:"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:35.622912Z","iopub.status.busy":"2022-09-04T17:20:35.622133Z","iopub.status.idle":"2022-09-04T17:20:35.820853Z","shell.execute_reply":"2022-09-04T17:20:35.819482Z","shell.execute_reply.started":"2022-09-04T17:20:35.622874Z"},"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:35.823680Z","iopub.status.busy":"2022-09-04T17:20:35.823343Z","iopub.status.idle":"2022-09-04T17:20:35.836229Z","shell.execute_reply":"2022-09-04T17:20:35.832509Z","shell.execute_reply.started":"2022-09-04T17:20:35.823652Z"},"trusted":true},"outputs":[],"source":["label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:17:33.656105Z","iopub.status.busy":"2022-09-04T17:17:33.655459Z","iopub.status.idle":"2022-09-04T17:19:25.869219Z","shell.execute_reply":"2022-09-04T17:19:25.868230Z","shell.execute_reply.started":"2022-09-04T17:17:33.656069Z"},"trusted":true},"outputs":[],"source":["model_name = \"bloom-560m\"\n","tokenizer = BloomTokenizerFast.from_pretrained(f\"bigscience/{model_name}\", add_prefix_space=True)\n","model = AutoModelForTokenClassification.from_pretrained(f\"bigscience/{model_name}\", num_labels=len(label_list))#.cuda()"]},{"cell_type":"markdown","metadata":{},"source":["## Define Data Collator"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:35.615127Z","iopub.status.busy":"2022-09-04T17:20:35.614018Z","iopub.status.idle":"2022-09-04T17:20:35.620697Z","shell.execute_reply":"2022-09-04T17:20:35.619590Z","shell.execute_reply.started":"2022-09-04T17:20:35.615098Z"},"trusted":true},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{},"source":["## Define Trainer"]},{"cell_type":"markdown","metadata":{},"source":["About the Model:\n","\n","see https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/modeling_utils.py#L829"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:49.161724Z","iopub.status.busy":"2022-09-04T17:20:49.161376Z","iopub.status.idle":"2022-09-04T17:20:49.174161Z","shell.execute_reply":"2022-09-04T17:20:49.173299Z","shell.execute_reply.started":"2022-09-04T17:20:49.161688Z"},"trusted":true},"outputs":[],"source":["print(\"Parameters:\", model.num_parameters())\n","print(\"Expected Input Dict:\", model.main_input_name )\n","\n","# Estimate FLOPS needed for one training example\n","sample = dataset[\"train\"][0]\n","sample[\"input_ids\"] = torch.Tensor(sample[\"input_ids\"])\n","flops_est = model.floating_point_ops(input_dict = sample, exclude_embeddings = False)\n","\n","print(\"FLOPS needed per Training Sample:\", flops_est )"]},{"cell_type":"markdown","metadata":{},"source":["**Define Optimizer:**\n","\n","See https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.Adafactor"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:49.176350Z","iopub.status.busy":"2022-09-04T17:20:49.175774Z","iopub.status.idle":"2022-09-04T17:20:49.181946Z","shell.execute_reply":"2022-09-04T17:20:49.180928Z","shell.execute_reply.started":"2022-09-04T17:20:49.176314Z"},"trusted":true},"outputs":[],"source":["#from transformers.optimization import Adafactor, AdafactorSchedule\n","\n","#optimizer = Adafactor(    \n","#        model.parameters(),\n","#        lr=1e-3,\n","#        eps=(1e-30, 1e-3),\n","#        clip_threshold=1.0,\n","#        decay_rate=-0.8,\n","#        beta1=None,\n","#        weight_decay=0.0,\n","#        relative_step=False,\n","#        scale_parameter=False,\n","#        warmup_init=False,\n","#    )\n","\n","#lr_scheduler = AdafactorSchedule(optimizer)"]},{"cell_type":"markdown","metadata":{},"source":["**Define Metrics:**\n","\n","See https://huggingface.co/course/chapter7/2#metrics"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:52.528408Z","iopub.status.busy":"2022-09-04T17:23:52.527563Z","iopub.status.idle":"2022-09-04T17:23:53.180502Z","shell.execute_reply":"2022-09-04T17:23:53.179321Z","shell.execute_reply.started":"2022-09-04T17:23:52.528364Z"},"trusted":true},"outputs":[],"source":["metric = load_metric(\"seqeval\")"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:54.086675Z","iopub.status.busy":"2022-09-04T17:23:54.086175Z","iopub.status.idle":"2022-09-04T17:23:54.108136Z","shell.execute_reply":"2022-09-04T17:23:54.107103Z","shell.execute_reply.started":"2022-09-04T17:23:54.086641Z"},"trusted":true},"outputs":[],"source":["example = dataset[\"train\"][150]\n","labels = [label_list[i] for i in example[f\"ner_tags\"]]\n","metric.compute(predictions=[labels], references=[labels])"]},{"cell_type":"markdown","metadata":{},"source":["Set correct class labels:"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:54.949476Z","iopub.status.busy":"2022-09-04T17:23:54.949090Z","iopub.status.idle":"2022-09-04T17:23:54.956761Z","shell.execute_reply":"2022-09-04T17:23:54.955061Z","shell.execute_reply.started":"2022-09-04T17:23:54.949441Z"},"trusted":true},"outputs":[],"source":["label_names = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n","\n","id2label = {id : label for id, label in enumerate(label_names)}\n","label2id = {label: id for id, label in enumerate(label_names)}\n","\n","model.config.id2label = id2label\n","model.config.label2id = label2id"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:55.633459Z","iopub.status.busy":"2022-09-04T17:23:55.633071Z","iopub.status.idle":"2022-09-04T17:23:55.639657Z","shell.execute_reply":"2022-09-04T17:23:55.638357Z","shell.execute_reply.started":"2022-09-04T17:23:55.633426Z"},"trusted":true},"source":["Define callback function to evaluate the model:"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:55.991541Z","iopub.status.busy":"2022-09-04T17:23:55.990958Z","iopub.status.idle":"2022-09-04T17:23:56.000348Z","shell.execute_reply":"2022-09-04T17:23:55.998152Z","shell.execute_reply.started":"2022-09-04T17:23:55.991502Z"},"trusted":true},"outputs":[],"source":["label_names = model.config.id2label\n","\n","def compute_metrics(eval_preds):\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","\n","    true_labels = [[label_names[l] for l in label  if l != -100] for label in labels]\n","    #true_predictions = [model.config.id2label[t.item()] for t in predictions]\n","    \n","    true_predictions = [\n","        [label_names[p] for (p, l) in zip(prediction, label)  if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }"]},{"cell_type":"markdown","metadata":{},"source":["**Set further Training Arguments:**\n","\n","See https://huggingface.co/docs/transformers/v4.21.2/en/main_classes/trainer#transformers.TrainingArguments"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:56.904243Z","iopub.status.busy":"2022-09-04T17:23:56.903622Z","iopub.status.idle":"2022-09-04T17:23:56.925735Z","shell.execute_reply":"2022-09-04T17:23:56.924817Z","shell.execute_reply.started":"2022-09-04T17:23:56.904204Z"},"trusted":true},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"./results\",\n","    save_strategy= \"no\",# \"epoch\",\n","    #save_steps = 2000,\n","    remove_unused_columns = True,\n","    evaluation_strategy=\"epoch\",\n","    #eval_steps = 2000,\n","    #load_best_model_at_end=True,\n","    logging_strategy = \"steps\",\n","    logging_steps = 100,\n","    learning_rate=2e-5,\n","    auto_find_batch_size = True,\n","    #per_device_train_batch_size=4,\n","    #per_device_eval_batch_size=2,\n","    #gradient_accumulation_steps=4,\n","    #optim=\"adamw_torch\",\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    report_to=\"none\",\n","    fp16=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    #optimizers=(optimizer, lr_scheduler),\n","    compute_metrics=compute_metrics\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Train Model\n","\n","GPU used by Kaggle: https://www.nvidia.com/de-de/data-center/tesla-p100/"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:58.843624Z","iopub.status.busy":"2022-09-04T17:23:58.842692Z","iopub.status.idle":"2022-09-04T17:23:59.968744Z","shell.execute_reply":"2022-09-04T17:23:59.967514Z","shell.execute_reply.started":"2022-09-04T17:23:58.843577Z"},"trusted":true},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:23:59.972792Z","iopub.status.busy":"2022-09-04T17:23:59.971064Z","iopub.status.idle":"2022-09-04T17:40:47.605893Z","shell.execute_reply":"2022-09-04T17:40:47.604419Z","shell.execute_reply.started":"2022-09-04T17:23:59.972745Z"},"trusted":true},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["eval_results = trainer.evaluate()\n","print(f\"Eval Loss: {eval_results['eval_loss']}\")"]},{"cell_type":"markdown","metadata":{},"source":["**Saving the fine tuned model & tokenizer:**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["trainer.save_model(f'./results/checkpoint-final/')"]},{"cell_type":"markdown","metadata":{},"source":["**Calculate Accuracy:**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["predictions, labels, _ = trainer.predict(dataset[\"test\"])\n","predictions = np.argmax(predictions, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["true_labels = [\n","    [label_names[l] for l in label  if l != -100] \n","    for label in labels\n","]\n","\n","true_predictions = [\n","    [label_names[p] for (p, l) in zip(prediction, label)  if l != -100]\n","    for prediction, label in zip(predictions, labels)\n","]\n","\n","results = metric.compute(predictions=true_predictions, references=true_labels)\n","pprint(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["training_history = trainer.state.log_history\n","\n","# Training\n","epochs = [epoch.get(\"epoch\") for epoch in  training_history if epoch.get(\"loss\") is not None]\n","steps = [epoch.get(\"step\") for epoch in  training_history if epoch.get(\"loss\") is not None]\n","loss = [epoch.get(\"loss\") for epoch in  training_history if epoch.get(\"loss\") is not None]\n","\n","# Eval\n","eval_epochs = [epoch.get(\"epoch\") for epoch in  training_history if epoch.get(\"eval_loss\") is not None]\n","eval_steps = [epoch.get(\"step\") for epoch in  training_history if epoch.get(\"eval_loss\") is not None]\n","eval_loss = [epoch.get(\"eval_loss\") for epoch in  training_history if epoch.get(\"eval_loss\") is not None]\n","eval_precision = [epoch.get(\"eval_precision\") for epoch in  training_history if epoch.get(\"eval_precision\") is not None]\n","eval_recall = [epoch.get(\"eval_recall\") for epoch in  training_history if epoch.get(\"eval_recall\") is not None]\n","eval_f1 = [epoch.get(\"eval_recall\") for epoch in  training_history if epoch.get(\"eval_f1\") is not None]\n","eval_accuracy = [epoch.get(\"eval_accuracy\") for epoch in  training_history if epoch.get(\"eval_accuracy\") is not None]\n","\n","eval_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["p = sns.lineplot( x=steps, y=loss)\n","p.set_xlabel(\"Training Steps\")\n","p.set_ylabel(\"Loss\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["p = sns.lineplot(x=eval_steps, y=eval_loss)\n","p.set_xlabel(\"Eval Steps\")\n","p.set_ylabel(\"Loss\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["p = sns.lineplot(x=eval_steps, y=eval_accuracy)\n","p.set_xlabel(\"Eval Steps\")\n","p.set_ylabel(\"Loss\")"]},{"cell_type":"markdown","metadata":{},"source":["## Use Fine-tuned Model:"]},{"cell_type":"markdown","metadata":{},"source":["Load checkpoint:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_tuned = BloomForTokenClassification.from_pretrained(\"./results/checkpoint-final/\")"]},{"cell_type":"markdown","metadata":{},"source":["Set correct class labels:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["label_names = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n","\n","id2label = {id : label for id, label in enumerate(label_names)}\n","label2id = {label: id for id, label in enumerate(label_names)}\n","\n","model_tuned.config.id2label = id2label\n","model_tuned.config.label2id = label2id"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_tuned.config.id2label"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["text = \"Für Richard Phillips Feynman war es immer wichtig in New York, die unanschaulichen Gesetzmäßigkeiten der Quantenphysik Laien und Studenten nahezubringen und verständlich zu machen.\"\n","\n","inputs = tokenizer(\n","    text, \n","    add_special_tokens=False, return_tensors=\"pt\"\n",")\n","\n","with torch.no_grad():\n","    logits = model_tuned(**inputs).logits\n","\n","predicted_token_class_ids = logits.argmax(-1)\n","\n","# Note that tokens are classified rather then input words which means that\n","# there might be more predicted token classes than words.\n","# Multiple token classes might account for the same word\n","predicted_tokens_classes = [model_tuned.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n","predicted_tokens_classes"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["text = \"In December 1903 in France the Royal Swedish Academy of Sciences awarded Pierre Curie, Marie Curie, and Henri Becquerel the Nobel Prize in Physics\"\n","\n","inputs = tokenizer(\n","    text, \n","    add_special_tokens=False, return_tensors=\"pt\"\n",")\n","\n","with torch.no_grad():\n","    logits = model_tuned(**inputs).logits\n","\n","predicted_token_class_ids = logits.argmax(-1)\n","\n","# Note that tokens are classified rather then input words which means that\n","# there might be more predicted token classes than words.\n","# Multiple token classes might account for the same word\n","predicted_tokens_classes = [model_tuned.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n","predicted_tokens_classes"]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.4 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"72a3b8d639f2747ab586194b4ca9ce337f16fb375d739e56b4629a0a8f86dcaa"}}},"nbformat":4,"nbformat_minor":4}
