{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning RoBERTa for NER: Train Model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:17:23.428297Z",
     "iopub.status.busy": "2022-09-04T17:17:23.427338Z",
     "iopub.status.idle": "2022-09-04T17:17:31.843100Z",
     "shell.execute_reply": "2022-09-04T17:17:31.842031Z",
     "shell.execute_reply.started": "2022-09-04T17:17:23.427831Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (BertTokenizerFast,\n",
    "                          RobertaTokenizerFast,\n",
    "                          AutoTokenizer,\n",
    "                          BertForTokenClassification,\n",
    "                          RobertaForTokenClassification,\n",
    "                          DataCollatorForTokenClassification, \n",
    "                          AutoModelForTokenClassification, \n",
    "                          TrainingArguments, Trainer)\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets, DatasetDict\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dill as pickle\n",
    "import torch\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/dataset_processed.pkl\"\n",
    "with open(data_path, 'rb') as pickle_file:\n",
    "    dataset = pickle.load(file=pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Das',\n",
       "  'Beispiel',\n",
       "  'der',\n",
       "  'Wirtschaft',\n",
       "  'Japans',\n",
       "  'zeigt',\n",
       "  ',',\n",
       "  'dass',\n",
       "  'dies',\n",
       "  'Jahre',\n",
       "  'oder',\n",
       "  'Jahrzehnte',\n",
       "  'dauern',\n",
       "  'kann',\n",
       "  '.'],\n",
       " 'ner_tags': [0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'langs': ['de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de',\n",
       "  'de'],\n",
       " 'spans': ['ORG: Wirtschaft Japans'],\n",
       " 'input_ids': [1858,\n",
       "  24212,\n",
       "  122,\n",
       "  96913,\n",
       "  15758,\n",
       "  7,\n",
       "  35615,\n",
       "  6,\n",
       "  4,\n",
       "  1421,\n",
       "  14792,\n",
       "  12241,\n",
       "  1367,\n",
       "  182205,\n",
       "  13,\n",
       "  201560,\n",
       "  1876,\n",
       "  6,\n",
       "  5],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [0, 0, 0, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information about model variants can be found here: https://huggingface.co/docs/transformers/model_doc/roberta\n",
    "\n",
    "Load Model which can be finetuned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:20:35.622912Z",
     "iopub.status.busy": "2022-09-04T17:20:35.622133Z",
     "iopub.status.idle": "2022-09-04T17:20:35.820853Z",
     "shell.execute_reply": "2022-09-04T17:20:35.819482Z",
     "shell.execute_reply.started": "2022-09-04T17:20:35.622874Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:20:35.823680Z",
     "iopub.status.busy": "2022-09-04T17:20:35.823343Z",
     "iopub.status.idle": "2022-09-04T17:20:35.836229Z",
     "shell.execute_reply": "2022-09-04T17:20:35.832509Z",
     "shell.execute_reply.started": "2022-09-04T17:20:35.823652Z"
    }
   },
   "outputs": [],
   "source": [
    "label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:17:33.656105Z",
     "iopub.status.busy": "2022-09-04T17:17:33.655459Z",
     "iopub.status.idle": "2022-09-04T17:19:25.869219Z",
     "shell.execute_reply": "2022-09-04T17:19:25.868230Z",
     "shell.execute_reply.started": "2022-09-04T17:17:33.656069Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"xlm-roberta-large\" #\"bert-base-multilingual-cased\" #xlm-roberta-large\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{model_name}\", add_prefix_space=True) #AutoTokenizer(use_fast = True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(f\"{model_name}\", num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:20:35.615127Z",
     "iopub.status.busy": "2022-09-04T17:20:35.614018Z",
     "iopub.status.idle": "2022-09-04T17:20:35.620697Z",
     "shell.execute_reply": "2022-09-04T17:20:35.619590Z",
     "shell.execute_reply.started": "2022-09-04T17:20:35.615098Z"
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the Model:\n",
    "\n",
    "see https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/modeling_utils.py#L829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:20:49.161724Z",
     "iopub.status.busy": "2022-09-04T17:20:49.161376Z",
     "iopub.status.idle": "2022-09-04T17:20:49.174161Z",
     "shell.execute_reply": "2022-09-04T17:20:49.173299Z",
     "shell.execute_reply.started": "2022-09-04T17:20:49.161688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 558848007\n",
      "Expected Input Dict: input_ids\n",
      "FLOPS needed per Training Sample: 63708672798\n"
     ]
    }
   ],
   "source": [
    "print(\"Parameters:\", model.num_parameters())\n",
    "print(\"Expected Input Dict:\", model.main_input_name )\n",
    "\n",
    "# Estimate FLOPS needed for one training example\n",
    "sample = dataset[\"train\"][0]\n",
    "sample[\"input_ids\"] = torch.Tensor(sample[\"input_ids\"])\n",
    "flops_est = model.floating_point_ops(input_dict = sample, exclude_embeddings = False)\n",
    "\n",
    "print(\"FLOPS needed per Training Sample:\", flops_est )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'spans', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 40000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'spans', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags', 'langs', 'spans', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Optimizer:**\n",
    "\n",
    "See https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.Adafactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:20:49.176350Z",
     "iopub.status.busy": "2022-09-04T17:20:49.175774Z",
     "iopub.status.idle": "2022-09-04T17:20:49.181946Z",
     "shell.execute_reply": "2022-09-04T17:20:49.180928Z",
     "shell.execute_reply.started": "2022-09-04T17:20:49.176314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 1250.0\n"
     ]
    }
   ],
   "source": [
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 16\n",
    "num_reports = 10\n",
    "\n",
    "# A training step is one gradient update. In one step batch_size examples are processed.\n",
    "# An epoch consists of one full cycle through the training data. \n",
    "# This is usually many steps. As an example, if you have 2,000 images and use\n",
    "# a batch size of 10 an epoch consists of:\n",
    "gpu_count = torch.cuda.device_count()\n",
    "num_steps = (len(dataset[\"train\"]) / batch_size / gpu_count) * num_epochs\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-6, weight_decay=0.01, no_deprecation_warning= True)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps= num_steps \n",
    ")\n",
    "\n",
    "print(\"Steps:\", num_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Log and Eval Interval:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval interval: 125\n"
     ]
    }
   ],
   "source": [
    "report_steps = math.floor(num_steps / num_reports)\n",
    "print(\"Eval interval:\", report_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Metrics:**\n",
    "\n",
    "See https://huggingface.co/course/chapter7/2#metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:52.528408Z",
     "iopub.status.busy": "2022-09-04T17:23:52.527563Z",
     "iopub.status.idle": "2022-09-04T17:23:53.180502Z",
     "shell.execute_reply": "2022-09-04T17:23:53.179321Z",
     "shell.execute_reply.started": "2022-09-04T17:23:52.528364Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1925177/152412463.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:54.086675Z",
     "iopub.status.busy": "2022-09-04T17:23:54.086175Z",
     "iopub.status.idle": "2022-09-04T17:23:54.108136Z",
     "shell.execute_reply": "2022-09-04T17:23:54.107103Z",
     "shell.execute_reply.started": "2022-09-04T17:23:54.086641Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"train\"][150]\n",
    "labels = [label_list[i] for i in example[f\"labels\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set correct class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:54.949476Z",
     "iopub.status.busy": "2022-09-04T17:23:54.949090Z",
     "iopub.status.idle": "2022-09-04T17:23:54.956761Z",
     "shell.execute_reply": "2022-09-04T17:23:54.955061Z",
     "shell.execute_reply.started": "2022-09-04T17:23:54.949441Z"
    }
   },
   "outputs": [],
   "source": [
    "label_names = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "\n",
    "id2label = {id : label for id, label in enumerate(label_names)}\n",
    "label2id = {label: id for id, label in enumerate(label_names)}\n",
    "\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:55.633459Z",
     "iopub.status.busy": "2022-09-04T17:23:55.633071Z",
     "iopub.status.idle": "2022-09-04T17:23:55.639657Z",
     "shell.execute_reply": "2022-09-04T17:23:55.638357Z",
     "shell.execute_reply.started": "2022-09-04T17:23:55.633426Z"
    }
   },
   "source": [
    "Define callback function to evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:55.991541Z",
     "iopub.status.busy": "2022-09-04T17:23:55.990958Z",
     "iopub.status.idle": "2022-09-04T17:23:56.000348Z",
     "shell.execute_reply": "2022-09-04T17:23:55.998152Z",
     "shell.execute_reply.started": "2022-09-04T17:23:55.991502Z"
    }
   },
   "outputs": [],
   "source": [
    "label_names = model.config.id2label\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    true_labels = [[label_names[l] for l in label  if l != -100] for label in labels]\n",
    "    #true_predictions = [model.config.id2label[t.item()] for t in predictions]\n",
    "    \n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label)  if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove unnecessary columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns([\"tokens\", \"ner_tags\", \"langs\", \"spans\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set further Training Arguments:**\n",
    "\n",
    "See https://huggingface.co/docs/transformers/v4.21.2/en/main_classes/trainer#transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:56.904243Z",
     "iopub.status.busy": "2022-09-04T17:23:56.903622Z",
     "iopub.status.idle": "2022-09-04T17:23:56.925735Z",
     "shell.execute_reply": "2022-09-04T17:23:56.924817Z",
     "shell.execute_reply.started": "2022-09-04T17:23:56.904204Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps = report_steps,\n",
    "    remove_unused_columns = True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps = report_steps,\n",
    "    #load_best_model_at_end=True,\n",
    "    logging_strategy = \"steps\",\n",
    "    logging_steps = report_steps,\n",
    "    #learning_rate= 2e-5,\n",
    "    #auto_find_batch_size = True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    #optim=\"adamw_torch\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    #weight_decay=0.01,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "GPU used by Kaggle: https://www.nvidia.com/de-de/data-center/tesla-p100/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:58.843624Z",
     "iopub.status.busy": "2022-09-04T17:23:58.842692Z",
     "iopub.status.idle": "2022-09-04T17:23:59.968744Z",
     "shell.execute_reply": "2022-09-04T17:23:59.967514Z",
     "shell.execute_reply.started": "2022-09-04T17:23:58.843577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan  7 16:47:36 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.108.03   Driver Version: 510.108.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          Off  | 00000000:01:00.0 Off |                    0 |\n",
      "|  0%   43C    P0    78W / 300W |   2945MiB / 46068MiB |     42%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A40          Off  | 00000000:43:00.0 Off |                    0 |\n",
      "|  0%   43C    P8    35W / 300W |     26MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   1549808      G   /usr/lib/xorg/Xorg                 23MiB |\n",
      "|    0   N/A  N/A   1925177      C   ...s/transformers/bin/python     2919MiB |\n",
      "|    1   N/A  N/A   1549808      G   /usr/lib/xorg/Xorg                 23MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T17:23:59.972792Z",
     "iopub.status.busy": "2022-09-04T17:23:59.971064Z",
     "iopub.status.idle": "2022-09-04T17:40:47.605893Z",
     "shell.execute_reply": "2022-09-04T17:40:47.604419Z",
     "shell.execute_reply.started": "2022-09-04T17:23:59.972745Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 40000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1250\n",
      "  Number of trainable parameters = 558848007\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 22:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.142200</td>\n",
       "      <td>0.593166</td>\n",
       "      <td>0.469493</td>\n",
       "      <td>0.517918</td>\n",
       "      <td>0.492518</td>\n",
       "      <td>0.812434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.542300</td>\n",
       "      <td>0.384173</td>\n",
       "      <td>0.667477</td>\n",
       "      <td>0.698618</td>\n",
       "      <td>0.682692</td>\n",
       "      <td>0.881813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.327806</td>\n",
       "      <td>0.745336</td>\n",
       "      <td>0.768184</td>\n",
       "      <td>0.756587</td>\n",
       "      <td>0.906123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.361300</td>\n",
       "      <td>0.302047</td>\n",
       "      <td>0.758416</td>\n",
       "      <td>0.771904</td>\n",
       "      <td>0.765100</td>\n",
       "      <td>0.911014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.284788</td>\n",
       "      <td>0.781934</td>\n",
       "      <td>0.781281</td>\n",
       "      <td>0.781607</td>\n",
       "      <td>0.916402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.327300</td>\n",
       "      <td>0.282079</td>\n",
       "      <td>0.779709</td>\n",
       "      <td>0.801420</td>\n",
       "      <td>0.790415</td>\n",
       "      <td>0.916028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>0.290900</td>\n",
       "      <td>0.271596</td>\n",
       "      <td>0.793539</td>\n",
       "      <td>0.801040</td>\n",
       "      <td>0.797272</td>\n",
       "      <td>0.921077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.248130</td>\n",
       "      <td>0.808497</td>\n",
       "      <td>0.814498</td>\n",
       "      <td>0.811486</td>\n",
       "      <td>0.926583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>0.269700</td>\n",
       "      <td>0.246678</td>\n",
       "      <td>0.813195</td>\n",
       "      <td>0.808101</td>\n",
       "      <td>0.810640</td>\n",
       "      <td>0.925816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.239190</td>\n",
       "      <td>0.818208</td>\n",
       "      <td>0.817307</td>\n",
       "      <td>0.817757</td>\n",
       "      <td>0.928426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-125\n",
      "Configuration saved in ./results/checkpoint-125/config.json\n",
      "Model weights saved in ./results/checkpoint-125/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-125/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-125/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-250\n",
      "Configuration saved in ./results/checkpoint-250/config.json\n",
      "Model weights saved in ./results/checkpoint-250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-250/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-250/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-375\n",
      "Configuration saved in ./results/checkpoint-375/config.json\n",
      "Model weights saved in ./results/checkpoint-375/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-375/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-375/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-625\n",
      "Configuration saved in ./results/checkpoint-625/config.json\n",
      "Model weights saved in ./results/checkpoint-625/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-625/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-625/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-750\n",
      "Configuration saved in ./results/checkpoint-750/config.json\n",
      "Model weights saved in ./results/checkpoint-750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-750/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-875\n",
      "Configuration saved in ./results/checkpoint-875/config.json\n",
      "Model weights saved in ./results/checkpoint-875/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-875/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-875/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-1125\n",
      "Configuration saved in ./results/checkpoint-1125/config.json\n",
      "Model weights saved in ./results/checkpoint-1125/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1125/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1125/special_tokens_map.json\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to ./results/checkpoint-1250\n",
      "Configuration saved in ./results/checkpoint-1250/config.json\n",
      "Model weights saved in ./results/checkpoint-1250/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1250/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1250/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.42325708923339844, metrics={'train_runtime': 1379.2866, 'train_samples_per_second': 29.0, 'train_steps_per_second': 0.906, 'total_flos': 3160037154846528.0, 'train_loss': 0.42325708923339844, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 32\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/transformers/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss: 0.23919013142585754\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Eval Loss: {eval_results['eval_loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving the fine tuned model & tokenizer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-final/\n",
      "Configuration saved in ./results/checkpoint-final/config.json\n",
      "Model weights saved in ./results/checkpoint-final/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-final/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-final/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(f'./results/checkpoint-final/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Training History:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./results/checkpoint-final/training_args.pkl\"\n",
    "with open(data_path, 'wb') as pickle_file:\n",
    "    pickle.dump(obj = trainer.args, file=pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./results/checkpoint-final/training_history.pkl\"\n",
    "with open(data_path, 'wb') as pickle_file:\n",
    "    pickle.dump(obj = trainer.state, file=pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Accuracy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 20000\n",
      "  Batch size = 32\n"
     ]
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(dataset[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LOC': {'f1': 0.8512989302927002,\n",
      "         'number': 21063,\n",
      "         'precision': 0.843191132637854,\n",
      "         'recall': 0.8595641646489104},\n",
      " 'ORG': {'f1': 0.732703281027104,\n",
      "         'number': 16972,\n",
      "         'precision': 0.7392060446150156,\n",
      "         'recall': 0.7263139288239453},\n",
      " 'PER': {'f1': 0.8670305901740653,\n",
      "         'number': 14649,\n",
      "         'precision': 0.872150849564857,\n",
      "         'recall': 0.8619701003481466},\n",
      " 'overall_accuracy': 0.9284256743382577,\n",
      " 'overall_f1': 0.8177570980913494,\n",
      " 'overall_precision': 0.8182077300193821,\n",
      " 'overall_recall': 0.8173069622655835}\n"
     ]
    }
   ],
   "source": [
    "true_labels = [\n",
    "    [label_names[l] for l in label  if l != -100] \n",
    "    for label in labels\n",
    "]\n",
    "\n",
    "true_predictions = [\n",
    "    [label_names[p] for (p, l) in zip(prediction, label)  if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c97fe7fd1c286c8419eb1bea19acb7c2170e8f8ba541cc471414a0cdc49a8156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
