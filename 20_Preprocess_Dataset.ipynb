{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning RoBERTa for NER: Preprocess Corpus\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:10.031751Z",
     "iopub.status.busy": "2022-09-04T22:12:10.031387Z",
     "iopub.status.idle": "2022-09-04T22:12:11.866792Z",
     "shell.execute_reply": "2022-09-04T22:12:11.865515Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (BertTokenizerFast,\n",
    "                          RobertaTokenizerFast,\n",
    "                          AutoTokenizer,\n",
    "                          BertForTokenClassification,\n",
    "                          RobertaForTokenClassification,\n",
    "                          DataCollatorForTokenClassification, \n",
    "                          AutoModelForTokenClassification, \n",
    "                          TrainingArguments, Trainer)\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "import pickle\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model and Tokenizer:**\n",
    "\n",
    "Information about model variants can be found here: https://huggingface.co/docs/transformers/model_doc/roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:11.871856Z",
     "iopub.status.busy": "2022-09-04T22:12:11.871520Z",
     "iopub.status.idle": "2022-09-04T22:12:15.832555Z",
     "shell.execute_reply": "2022-09-04T22:12:15.831245Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"xlm-roberta-large\" #\"bert-base-multilingual-cased\" #xlm-roberta-large\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{model_name}\", add_prefix_space=True) #AutoTokenizer(use_fast = True)\n",
    "#model = AutoModelForTokenClassification.from_pretrained(f\"{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:15.837803Z",
     "iopub.status.busy": "2022-09-04T22:12:15.837570Z",
     "iopub.status.idle": "2022-09-04T22:12:15.846191Z",
     "shell.execute_reply": "2022-09-04T22:12:15.844952Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"./data/dataset_multilingual.pkl\"\n",
    "with open(data_path, 'rb') as pickle_file:\n",
    "    dataset = pickle.load(file=pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize a Single Sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:15.851458Z",
     "iopub.status.busy": "2022-09-04T22:12:15.851264Z",
     "iopub.status.idle": "2022-09-04T22:12:15.857695Z",
     "shell.execute_reply": "2022-09-04T22:12:15.856662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁List', '▁of', '▁Fly', 'w', 'heel', '▁', ',', '▁Sh', 'y', 'ster', '▁', ',', '▁and', '▁Fly', 'w', 'heel', '▁(', '▁1990', '▁radio', '▁series', '▁)', '▁episode', 's']\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][50]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True,add_special_tokens=False)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample after Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:15.862455Z",
     "iopub.status.busy": "2022-09-04T22:12:15.862264Z",
     "iopub.status.idle": "2022-09-04T22:12:15.873277Z",
     "shell.execute_reply": "2022-09-04T22:12:15.872211Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [32036, 111, 25066, 434, 144009, 6, 4, 7525, 53, 1515, 6, 4, 136, 25066, 434, 144009, 15, 11704, 5977, 36549, 1388, 50094, 7], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:15.877923Z",
     "iopub.status.busy": "2022-09-04T22:12:15.877734Z",
     "iopub.status.idle": "2022-09-04T22:12:15.883845Z",
     "shell.execute_reply": "2022-09-04T22:12:15.882930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 6, 7, 7, 7, 8, 9, 10, 11, 12, 13, 13]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_input.word_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:15.888511Z",
     "iopub.status.busy": "2022-09-04T22:12:15.888285Z",
     "iopub.status.idle": "2022-09-04T22:12:15.893688Z",
     "shell.execute_reply": "2022-09-04T22:12:15.892683Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenizeInputs(inputs):\n",
    "    \n",
    "    tokenized_inputs = tokenizer(inputs[\"tokens\"], max_length = 512, truncation=True, is_split_into_words=True, add_special_tokens=False)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    ner_tags = inputs[\"ner_tags\"]\n",
    "    labels = [ner_tags[word_id] for word_id in word_ids]\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:15.898275Z",
     "iopub.status.busy": "2022-09-04T22:12:15.898020Z",
     "iopub.status.idle": "2022-09-04T22:12:15.904166Z",
     "shell.execute_reply": "2022-09-04T22:12:15.903361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [32036, 111, 5369, 23, 30089], 'attention_mask': [1, 1, 1, 1, 1], 'labels': [3, 4, 4, 4, 4]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"train\"][100]\n",
    "tokenizeInputs(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:15.909420Z",
     "iopub.status.busy": "2022-09-04T22:12:15.909162Z",
     "iopub.status.idle": "2022-09-04T22:12:16.548707Z",
     "shell.execute_reply": "2022-09-04T22:12:16.547387Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/pop529700/.cache/huggingface/datasets/wikiann/en/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e/cache-9779b6b3866e6c99.arrow\n",
      "Loading cached processed dataset at /home/pop529700/.cache/huggingface/datasets/wikiann/en/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e/cache-eeb09de258f3aa4e.arrow\n",
      "Loading cached processed dataset at /home/pop529700/.cache/huggingface/datasets/wikiann/en/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e/cache-300a86ff532f965f.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenizeInputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffle Dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:16.553826Z",
     "iopub.status.busy": "2022-09-04T22:12:16.553614Z",
     "iopub.status.idle": "2022-09-04T22:12:16.600567Z",
     "shell.execute_reply": "2022-09-04T22:12:16.599295Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count of Tokens in the Training Set:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:16.605847Z",
     "iopub.status.busy": "2022-09-04T22:12:16.605642Z",
     "iopub.status.idle": "2022-09-04T22:12:37.145913Z",
     "shell.execute_reply": "2022-09-04T22:12:37.144495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in Training Set: 12458\n"
     ]
    }
   ],
   "source": [
    "token_count = 0\n",
    "for sample in tokenized_dataset[\"train\"]:\n",
    "    token_count = token_count + len(sample[\"labels\"])\n",
    "    \n",
    "print(\"Tokens in Training Set:\", token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove unnecessary columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:37.151411Z",
     "iopub.status.busy": "2022-09-04T22:12:37.151166Z",
     "iopub.status.idle": "2022-09-04T22:12:37.162851Z",
     "shell.execute_reply": "2022-09-04T22:12:37.162161Z"
    }
   },
   "outputs": [],
   "source": [
    "#tokenized_dataset = tokenized_dataset.remove_columns([\"tokens\", \"ner_tags\", \"langs\", \"spans\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save processed Dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-04T22:12:37.166629Z",
     "iopub.status.busy": "2022-09-04T22:12:37.166302Z",
     "iopub.status.idle": "2022-09-04T22:12:37.171926Z",
     "shell.execute_reply": "2022-09-04T22:12:37.170777Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = \"./data/dataset_processed.pkl\"\n",
    "with open(data_path, 'wb') as pickle_file:\n",
    "    pickle.dump(obj = tokenized_dataset, file=pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c97fe7fd1c286c8419eb1bea19acb7c2170e8f8ba541cc471414a0cdc49a8156"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
