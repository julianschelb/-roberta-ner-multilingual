{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Finetuning RoBERTa for NER: Use Model\n"," "]},{"cell_type":"markdown","metadata":{},"source":["***"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:17:23.428297Z","iopub.status.busy":"2022-09-04T17:17:23.427338Z","iopub.status.idle":"2022-09-04T17:17:31.843100Z","shell.execute_reply":"2022-09-04T17:17:31.842031Z","shell.execute_reply.started":"2022-09-04T17:17:23.427831Z"},"trusted":true},"outputs":[],"source":["from transformers import (BertTokenizerFast,\n","                          RobertaTokenizerFast,\n","                          AutoTokenizer,\n","                          BertForTokenClassification,\n","                          RobertaForTokenClassification,\n","                          DataCollatorForTokenClassification, \n","                          AutoModelForTokenClassification, \n","                          TrainingArguments, Trainer)\n","from datasets import load_dataset, load_metric, concatenate_datasets, DatasetDict\n","from pprint import pprint\n","import numpy as np\n","import pickle\n","import torch\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["## Load Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["data_path = \"./data/dataset_processed.pkl\"\n","with open(data_path, 'rb') as pickle_file:\n","    dataset = pickle.load(file=pickle_file)"]},{"cell_type":"markdown","metadata":{},"source":["## Load Model and Tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["Information about model variants can be found here: https://huggingface.co/docs/transformers/model_doc/roberta"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:35.622912Z","iopub.status.busy":"2022-09-04T17:20:35.622133Z","iopub.status.idle":"2022-09-04T17:20:35.820853Z","shell.execute_reply":"2022-09-04T17:20:35.819482Z","shell.execute_reply.started":"2022-09-04T17:20:35.622874Z"},"trusted":true},"outputs":[],"source":["import gc\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:20:35.823680Z","iopub.status.busy":"2022-09-04T17:20:35.823343Z","iopub.status.idle":"2022-09-04T17:20:35.836229Z","shell.execute_reply":"2022-09-04T17:20:35.832509Z","shell.execute_reply.started":"2022-09-04T17:20:35.823652Z"},"trusted":true},"outputs":[],"source":["label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-09-04T17:17:33.656105Z","iopub.status.busy":"2022-09-04T17:17:33.655459Z","iopub.status.idle":"2022-09-04T17:19:25.869219Z","shell.execute_reply":"2022-09-04T17:19:25.868230Z","shell.execute_reply.started":"2022-09-04T17:17:33.656069Z"},"trusted":true},"outputs":[],"source":["model_name = \"xlm-roberta-large\" #\"bert-base-multilingual-cased\" #xlm-roberta-large\n","tokenizer = AutoTokenizer.from_pretrained(f\"{model_name}\", add_prefix_space=True) #AutoTokenizer(use_fast = True)\n","#model = AutoModelForTokenClassification.from_pretrained(f\"{model_name}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Use Fine-tuned Model:"]},{"cell_type":"markdown","metadata":{},"source":["Load checkpoint:"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["model_tuned = AutoModelForTokenClassification.from_pretrained(\"./results/checkpoint-final/\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["XLMRobertaConfig {\n","  \"_name_or_path\": \"./results/checkpoint-final/\",\n","  \"architectures\": [\n","    \"XLMRobertaForTokenClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"O\",\n","    \"1\": \"B-PER\",\n","    \"2\": \"I-PER\",\n","    \"3\": \"B-ORG\",\n","    \"4\": \"I-ORG\",\n","    \"5\": \"B-LOC\",\n","    \"6\": \"I-LOC\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 4096,\n","  \"label2id\": {\n","    \"B-LOC\": 5,\n","    \"B-ORG\": 3,\n","    \"B-PER\": 1,\n","    \"I-LOC\": 6,\n","    \"I-ORG\": 4,\n","    \"I-PER\": 2,\n","    \"O\": 0\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 16,\n","  \"num_hidden_layers\": 24,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.21.3\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["model_tuned.config"]},{"cell_type":"markdown","metadata":{},"source":["Set correct class labels:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# label_names = dataset[\"train\"].features[f\"ner_tags\"].feature.names\n","\n","# id2label = {id : label for id, label in enumerate(label_names)}\n","# label2id = {label: id for id, label in enumerate(label_names)}\n","\n","# model_tuned.config.id2label = id2label\n","# model_tuned.config.label2id = label2id"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["{0: 'O',\n"," 1: 'B-PER',\n"," 2: 'I-PER',\n"," 3: 'B-ORG',\n"," 4: 'I-ORG',\n"," 5: 'B-LOC',\n"," 6: 'I-LOC'}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["model_tuned.config.id2label"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["def printPrediction(inputs, predictions, tokenizer):\n","    token_ids = list(inputs[\"input_ids\"][0])\n","    tokens_classes = predictions\n","    #results = []\n","\n","    for token_id, token_class in zip(token_ids, tokens_classes): \n","\n","        token_text = tokenizer.decode(int(token_id))\n","        #print(int(token_id),\"\\t\", token_text,\"\\t\", token_class)\n","        print(\"{: >10} {: >10} {: >10}\".format(int(token_id), token_text, token_class))\n","        #results.append((int(token_id), token_text, token_class))"]},{"cell_type":"code","execution_count":48,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["     10333        Für          O\n","     22758    Richard      B-PER\n","    165458    Phillip      I-PER\n","         7          s      I-PER\n","       563          F      I-PER\n","     28950        eyn      I-PER\n","       669        man      I-PER\n","      1631        war          O\n","       198         es          O\n","      3807      immer          O\n","     31097    wichtig          O\n","        23         in          O\n","      2356        New      B-LOC\n","      5753       York      I-LOC\n","         4          ,          O\n","        68        die          O\n","        51         un          O\n","        66         an          O\n","     32854      schau          O\n","     12512     lichen          O\n","     64086     Gesetz          O\n","     86756      mäßig          O\n","     21888       keit          O\n","        33         en          O\n","       122        der          O\n","     75344      Quant          O\n","        33         en          O\n","     34053        phy          O\n","     14383        sik          O\n","     12460        Lai          O\n","        33         en          O\n","       165        und          O\n","    151264  Studenten          O\n","    203578     nahezu          O\n","     74485    bringen          O\n","       165        und          O\n","    210041 verständlich          O\n","       404         zu          O\n","      8960     machen          O\n","         5          .          O\n"]}],"source":["text = \"Für Richard Phillips Feynman war es immer wichtig in New York, die unanschaulichen Gesetzmäßigkeiten der Quantenphysik Laien und Studenten nahezubringen und verständlich zu machen.\"\n","\n","inputs = tokenizer(\n","    text, \n","    add_special_tokens=False, return_tensors=\"pt\"\n",")\n","\n","with torch.no_grad():\n","    logits = model_tuned(**inputs).logits\n","\n","predicted_token_class_ids = logits.argmax(-1)\n","\n","# Note that tokens are classified rather then input words which means that\n","# there might be more predicted token classes than words.\n","# Multiple token classes might account for the same word\n","predicted_tokens_classes = [model_tuned.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n","\n","printPrediction(inputs, predicted_tokens_classes, tokenizer)"]},{"cell_type":"code","execution_count":49,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["       360         In          O\n","     14487   December          O\n","    106355       1903          O\n","        23         in          O\n","      9942     France      B-LOC\n","        70        the          O\n","     25674      Royal      B-ORG\n","    187951    Swedish      I-ORG\n","     62903    Academy      I-ORG\n","       111         of      I-ORG\n","     28745    Science      I-ORG\n","         7          s      I-ORG\n","     70318      award          O\n","       297         ed          O\n","     58807     Pierre      B-PER\n","     17065        Cur      I-PER\n","       478         ie      I-PER\n","         4          ,          O\n","     24479      Marie      B-PER\n","     17065        Cur      I-PER\n","       478         ie      I-PER\n","         4          ,          O\n","       136        and          O\n","     80640      Henri      B-PER\n","       873         Be      I-PER\n","       238          c      I-PER\n","       944        que      I-PER\n","      7962        rel      I-PER\n","        70        the          O\n","     34676      Nobel      B-ORG\n","      2319        Pri      I-ORG\n","       731         ze      I-ORG\n","        23         in      I-ORG\n","    165712     Physic      I-ORG\n","         7          s      I-ORG\n"]}],"source":["text = \"In December 1903 in France the Royal Swedish Academy of Sciences awarded Pierre Curie, Marie Curie, and Henri Becquerel the Nobel Prize in Physics\"\n","\n","inputs = tokenizer(\n","    text, \n","    add_special_tokens=False, return_tensors=\"pt\"\n",")\n","\n","with torch.no_grad():\n","    logits = model_tuned(**inputs).logits\n","\n","predicted_token_class_ids = logits.argmax(-1)\n","\n","# Note that tokens are classified rather then input words which means that\n","# there might be more predicted token classes than words.\n","# Multiple token classes might account for the same word\n","predicted_tokens_classes = [model_tuned.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n","\n","printPrediction(inputs, predicted_tokens_classes, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"44f1a40a401e6fa57b592d0b8560dd7ec3f203a86fbbd96af8799189ea4c1abc"}}},"nbformat":4,"nbformat_minor":4}
